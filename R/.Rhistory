#### 30 repetitions #####
results <- replicate(n = 3,
optim(c(-1, 0.2),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05),
simplify = F)
View(results)
?optim
#### 30 repetitions #####
results <- replicate(n = 3,
optim(c(-1, 0.2),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
method = "BFGS",
control = list(maxit = 200)),
simplify = F)
View(results)
# Getting the estimated optimal coefficients from each list.
# Thereafter, use the checking function to determine the c-statistic
# and prevalence belonging to the estimated betas
results_check <- apply(sapply(results, '[[', 1), 2, checking)
# What do the results look like?
apply(results_check, 1, summary)
# Taking the median of the coefficients,
# to reduce influence of potential outliers
par <- apply(sapply(results, '[[', 1), 1, median)
## Create validation dataset ##
n_val <- 100000 # setting n
X_val <- mvrnorm(n = n_val, mu = mu, Sigma = sigma) # create 10 predictor columns
dm_val <- cbind(1, X_val) # Putting the above in a data matrix, including intercept
## Use the checking function for the validation set:
checking_val(par = par)
#### 30 repetitions #####
results <- replicate(n = 3,
optim(c(-1, 0.2),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
control = list(maxit = 200)),
simplify = F)
# Getting the estimated optimal coefficients from each list.
# Thereafter, use the checking function to determine the c-statistic
# and prevalence belonging to the estimated betas
results_check <- apply(sapply(results, '[[', 1), 2, checking)
View(results)
View(results_check)
#### 30 repetitions #####
results <- replicate(n = 3,
optim(c(-1, 0.2),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
control = list(maxit = 300)),
simplify = F)
View(results)
# Getting the estimated optimal coefficients from each list.
# Thereafter, use the checking function to determine the c-statistic
# and prevalence belonging to the estimated betas
results_check <- apply(sapply(results, '[[', 1), 2, checking)
View(results_check)
#### 30 repetitions #####
results <- replicate(n = 3,
optim(c(0.5, 0.5),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
control = list(maxit = 300)),
simplify = F)
View(results)
# Getting the estimated optimal coefficients from each list.
# Thereafter, use the checking function to determine the c-statistic
# and prevalence belonging to the estimated betas
results_check <- apply(sapply(results, '[[', 1), 2, checking)
# What do the results look like?
apply(results_check, 1, summary)
# Taking the median of the coefficients,
# to reduce influence of potential outliers
par <- apply(sapply(results, '[[', 1), 1, median)
## Create validation dataset ##
n_val <- 100000 # setting n
X_val <- mvrnorm(n = n_val, mu = mu, Sigma = sigma) # create 10 predictor columns
dm_val <- cbind(1, X_val) # Putting the above in a data matrix, including intercept
## Use the checking function for the validation set:
checking_val(par = par)
#### 30 repetitions #####
results <- replicate(n = 30,
optim(c(0.5, 0.5),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
control = list(maxit = 300)),
simplify = F)
# Getting the estimated optimal coefficients from each list.
# Thereafter, use the checking function to determine the c-statistic
# and prevalence belonging to the estimated betas
results_check <- apply(sapply(results, '[[', 1), 2, checking)
#### 30 repetitions #####
system.time(results <- replicate(n = 30,
optim(c(0.5, 0.5),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
control = list(maxit = 300)),
simplify = F))
View(results)
# Getting the estimated optimal coefficients from each list.
# Thereafter, use the checking function to determine the c-statistic
# and prevalence belonging to the estimated betas
results_check <- apply(sapply(results, '[[', 1), 2, checking)
# What do the results look like?
apply(results_check, 1, summary)
# Taking the median of the coefficients,
# to reduce influence of potential outliers
par <- apply(sapply(results, '[[', 1), 1, median)
## Create validation dataset ##
n_val <- 100000 # setting n
X_val <- mvrnorm(n = n_val, mu = mu, Sigma = sigma) # create 10 predictor columns
dm_val <- cbind(1, X_val) # Putting the above in a data matrix, including intercept
## Use the checking function for the validation set:
checking_val(par = par)
#### 30 repetitions #####
system.time(results <- replicate(n = 30,
optim(c(0.5, 0.5),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
control = list(maxit = 500)),
simplify = F))
dist_R2_prev <- function(par,pref_R2, pref_prev, effects){
# par is a vector with initial guesses for both
# the intercept and the beta1-3 coefficients.
# However since beta1-3 have been restricted (half is strong, other half weak)
# only one value is necessary.
# Providing the beta0 and beta1-3 as specified in the par object.
if (effects == "half strong"){
# Half of the predictors is strong
dgm_par <- c(par[1], rep(par[2], round(n_pred/2)), rep((par[2]*3), round(n_pred/2)))
} else if (effects == "equal"){
# All predictors are equally strong
dgm_par <- c(par[1], rep(par[2], n_pred))
} else {
dgm_par <- c(par[1], rep(par[2], n_pred))
print("No effects supplied, so all are set to be equally strong")
}
# Obtain values for y based on Bernoulli distribution, with input p
#system.time(p <- exp(dm %*% dgm_par)/(1+exp(dm %*% dgm_par)))
p <- 1/(1+exp(-dm %*% dgm_par))# SAME THING, JUST SLIGHTLY LESS COMPUTATION
#system.time(y <- rbinom(length(p),1,p))
y <-as.numeric(p>runif(length(p))) # SAME THING, JUST SLIGHTLY FASTER
# Obtain observed values of c-statistic and
# average predicted probability of an event
obs_R2 <- pseudo_Rsqrs(p = p, y = y) # obtain R2cs based on p and y
obs_prev <- mean(y) # "Observed" prevalence
# Sum of absolute differences of both values:
#abs(obs_cstat-pref_cstat) + abs(obs_prev-pref_prev)
(obs_R2-pref_R2)^2 + (obs_prev-pref_prev)^2# alternative, not sure which one is better
}
#### 30 repetitions #####
system.time(results <- replicate(n = 10,
optim(c(0.5, 0.5),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
effects = "equal",
control = list(maxit = 500)),
simplify = F))
# Getting the estimated optimal coefficients from each list.
# Thereafter, use the checking function to determine the c-statistic
# and prevalence belonging to the estimated betas
results_check <- apply(sapply(results, '[[', 1), 2, checking, effects = "equal")
# Same idea as the checking function, however, using the validation data:
checking_val <- function(par, effects){
# Providing the beta0 and beta1-3 as specified in the par object.
if (effects == "half strong"){
# Half of the predictors is strong
dgm_par_val <- c(par[1], rep(par[2], round(n_pred/2)), rep((par[2]*3), round(n_pred/2)))
} else if (effects == "equal"){
# All predictors are equally strong
dgm_par_val <- c(par[1], rep(par[2], n_pred))
} else {
dgm_par_val <- c(par[1], rep(par[2], n_pred))
print("No effects supplied, so all are set to be equally strong")
}
p_val <- plogis(dm_val %*% dgm_par_val)
y_val <- rbinom(length(p_val),1,p_val)
# Obtain observed values
#obs_cstat <- c_stat2(preds = p_val, outcome = y_val) # obtain c-statistic based on p and y
obs_cstat <- fastAUC(p = p_val, y = y_val)
obs_prev <- mean(y_val) # THE OBSERVED PREVALENCE IS NOT A FUNCTION OF JUST THE INTERCEPT
c("cstat" = obs_cstat, "prev" = obs_prev)
}
checking <- function(par, effects){
## What do the observed prevalence and c-statistic look like?
# Providing the beta0 and beta1-3 as specified in the par object.
if (effects == "half strong"){
# Half of the predictors is strong
dgm_par <- c(par[1], rep(par[2], round(n_pred/2)), rep((par[2]*3), round(n_pred/2)))
} else if (effects == "equal"){
# All predictors are equally strong
dgm_par <- c(par[1], rep(par[2], n_pred))
} else {
dgm_par <- c(par[1], rep(par[2], n_pred))
print("No effects supplied, so all are set to be equally strong")
}
# Obtain values for y based on Bernoulli distribution, with input p
p <- plogis(dm %*% dgm_par)
y <- rbinom(length(p),1,p)
# Obtain observed values
#obs_cstat <- c_stat2(preds = p, outcome = y) # obtain c-statistic based on p and y
obs_cstat <- fastAUC(p = p, y = y)
obs_prev <- mean(y) # THE OBSERVED PREVALENCE IS NOT A FUNCTION OF JUST THE INTERCEPT
c("cstat" = obs_cstat, "prev" = obs_prev)
}
# Getting the estimated optimal coefficients from each list.
# Thereafter, use the checking function to determine the c-statistic
# and prevalence belonging to the estimated betas
results_check <- apply(sapply(results, '[[', 1), 2, checking, effects = "equal")
# What do the results look like?
apply(results_check, 1, summary)
# Taking the median of the coefficients,
# to reduce influence of potential outliers
par <- apply(sapply(results, '[[', 1), 1, median)
## Create validation dataset ##
n_val <- 100000 # setting n
X_val <- mvrnorm(n = n_val, mu = mu, Sigma = sigma) # create 10 predictor columns
dm_val <- cbind(1, X_val) # Putting the above in a data matrix, including intercept
# Same idea as the checking function, however, using the validation data:
checking_val <- function(par, effects){
# Providing the beta0 and beta1-3 as specified in the par object.
if (effects == "half strong"){
# Half of the predictors is strong
dgm_par_val <- c(par[1], rep(par[2], round(n_pred/2)), rep((par[2]*3), round(n_pred/2)))
} else if (effects == "equal"){
# All predictors are equally strong
dgm_par_val <- c(par[1], rep(par[2], n_pred))
} else {
dgm_par_val <- c(par[1], rep(par[2], n_pred))
print("No effects supplied, so all are set to be equally strong")
}
p_val <- plogis(dm_val %*% dgm_par_val)
y_val <- rbinom(length(p_val),1,p_val)
# Obtain observed values
#obs_cstat <- c_stat2(preds = p_val, outcome = y_val) # obtain c-statistic based on p and y
obs_cstat <- fastAUC(p = p_val, y = y_val)
obs_prev <- mean(y_val) # THE OBSERVED PREVALENCE IS NOT A FUNCTION OF JUST THE INTERCEPT
c("cstat" = obs_cstat, "prev" = obs_prev)
}
## Use the checking function for the validation set:
checking_val(par = par, effects = "equal")
#### 30 repetitions #####
system.time(results <- replicate(n = 10,
optim(c(-1, 0.2),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
effects = "equal",
control = list(maxit = 500)),
simplify = F))
View(results)
#### 30 repetitions #####
system.time(results <- replicate(n = 10,
optim(c(-3, 0.2),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
effects = "equal",
control = list(maxit = 500)),
simplify = F))
View(results)
# Getting the estimated optimal coefficients from each list.
# Thereafter, use the checking function to determine the c-statistic
# and prevalence belonging to the estimated betas
results_check <- apply(sapply(results, '[[', 1), 2, checking, effects = "equal")
#### 30 repetitions #####
system.time(results <- replicate(n = 10,
optim(c(-3, 0.2),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
effects = "equal",
control = list(maxit = 1000)),
simplify = F))
View(results)
#### 30 repetitions #####
system.time(results <- replicate(n = 10,
optim(c(-3, 0.2),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
effects = "equal",
method = "CG",
control = list(maxit = 1000)),
simplify = F))
View(results)
# Getting the estimated optimal coefficients from each list.
# Thereafter, use the checking function to determine the c-statistic
# and prevalence belonging to the estimated betas
results_check <- apply(sapply(results, '[[', 1), 2, checking, effects = "equal")
# What do the results look like?
apply(results_check, 1, summary)
# Taking the median of the coefficients,
# to reduce influence of potential outliers
par <- apply(sapply(results, '[[', 1), 1, median)
#### 30 repetitions #####
system.time(results <- replicate(n = 10,
optim(c(0.5, 0.5),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
effects = "equal",
method = "CG",
control = list(maxit = 1000)),
simplify = F))
# Getting the estimated optimal coefficients from each list.
# Thereafter, use the checking function to determine the c-statistic
# and prevalence belonging to the estimated betas
results_check <- apply(sapply(results, '[[', 1), 2, checking, effects = "equal")
# What do the results look like?
apply(results_check, 1, summary)
#### 30 repetitions #####
system.time(results <- replicate(n = 10,
optim(c(0.5, 0.5),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
effects = "equal",
control = list(maxit = 1000)),
simplify = F))
View(results)
# Getting the estimated optimal coefficients from each list.
# Thereafter, use the checking function to determine the c-statistic
# and prevalence belonging to the estimated betas
results_check <- apply(sapply(results, '[[', 1), 2, checking, effects = "equal")
# What do the results look like?
apply(results_check, 1, summary)
# Taking the median of the coefficients,
# to reduce influence of potential outliers
par <- apply(sapply(results, '[[', 1), 1, median)
## Create validation dataset ##
n_val <- 100000 # setting n
X_val <- mvrnorm(n = n_val, mu = mu, Sigma = sigma) # create 10 predictor columns
dm_val <- cbind(1, X_val) # Putting the above in a data matrix, including intercept
# Same idea as the checking function, however, using the validation data:
checking_val <- function(par, effects){
# Providing the beta0 and beta1-3 as specified in the par object.
if (effects == "half strong"){
# Half of the predictors is strong
dgm_par_val <- c(par[1], rep(par[2], round(n_pred/2)), rep((par[2]*3), round(n_pred/2)))
} else if (effects == "equal"){
# All predictors are equally strong
dgm_par_val <- c(par[1], rep(par[2], n_pred))
} else {
dgm_par_val <- c(par[1], rep(par[2], n_pred))
print("No effects supplied, so all are set to be equally strong")
}
p_val <- plogis(dm_val %*% dgm_par_val)
y_val <- rbinom(length(p_val),1,p_val)
# Obtain observed values
#obs_cstat <- c_stat2(preds = p_val, outcome = y_val) # obtain c-statistic based on p and y
obs_cstat <- fastAUC(p = p_val, y = y_val)
obs_prev <- mean(y_val) # THE OBSERVED PREVALENCE IS NOT A FUNCTION OF JUST THE INTERCEPT
c("cstat" = obs_cstat, "prev" = obs_prev)
}
## Use the checking function for the validation set:
checking_val(par = par, effects = "equal")
#### 30 repetitions #####
system.time(results <- replicate(n = 10,
optim(c(0.5, 0.5),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
effects = "equal",
method = "SANN"
control = list(maxit = 1000)),
simplify = F))
#### 30 repetitions #####
system.time(results <- replicate(n = 10,
optim(c(0.5, 0.5),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
effects = "equal",
method = "SANN",
control = list(maxit = 1000)),
simplify = F))
View(results)
#### 30 repetitions #####
system.time(results <- replicate(n = 10,
optim(c(0.5, 0.5),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
effects = "equal",
method = "BFGS",
control = list(maxit = 1000)),
simplify = F))
View(results)
#### 30 repetitions #####
system.time(results <- replicate(n = 10,
optim(c(0.5, 0.5),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
effects = "equal",
method = "Brent",
control = list(maxit = 1000)),
simplify = F))
#### 30 repetitions #####
system.time(results <- replicate(n = 10,
optim(c(0.5, 0.5),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
effects = "equal",
method = "L-BFGS-B",
control = list(maxit = 1000)),
simplify = F))
View(results)
#### 30 repetitions #####
system.time(results <- replicate(n = 10,
optim(c(0.5, 0.5),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
effects = "equal",
control = list(maxit = 1000)),
simplify = F))
View(results)
# Getting the estimated optimal coefficients from each list.
# Thereafter, use the checking function to determine the c-statistic
# and prevalence belonging to the estimated betas
results_check <- apply(sapply(results, '[[', 1), 2, checking, effects = "equal")
# What do the results look like?
apply(results_check, 1, summary)
# Taking the median of the coefficients,
# to reduce influence of potential outliers
par <- apply(sapply(results, '[[', 1), 1, median)
## Create validation dataset ##
n_val <- 100000 # setting n
X_val <- mvrnorm(n = n_val, mu = mu, Sigma = sigma) # create 10 predictor columns
dm_val <- cbind(1, X_val) # Putting the above in a data matrix, including intercept
# Same idea as the checking function, however, using the validation data:
checking_val <- function(par, effects = c("half strong", "equal")){
# Providing the beta0 and beta1-3 as specified in the par object.
if (effects == "half strong"){
# Half of the predictors is strong
dgm_par_val <- c(par[1], rep(par[2], round(n_pred/2)), rep((par[2]*3), round(n_pred/2)))
} else if (effects == "equal"){
# All predictors are equally strong
dgm_par_val <- c(par[1], rep(par[2], n_pred))
} else {
dgm_par_val <- c(par[1], rep(par[2], n_pred))
print("No effects supplied, so all are set to be equally strong")
}
p_val <- plogis(dm_val %*% dgm_par_val)
y_val <- rbinom(length(p_val),1,p_val)
# Obtain observed values
#obs_cstat <- c_stat2(preds = p_val, outcome = y_val) # obtain c-statistic based on p and y
obs_cstat <- fastAUC(p = p_val, y = y_val)
obs_prev <- mean(y_val) # THE OBSERVED PREVALENCE IS NOT A FUNCTION OF JUST THE INTERCEPT
c("cstat" = obs_cstat, "prev" = obs_prev)
}
## Use the checking function for the validation set:
checking_val(par = par, effects = "equal")
#### 30 repetitions #####
system.time(results <- replicate(n = 10,
optim(c(-2, 0.2),
dist_R2_prev,
pref_R2 = R2[1],
pref_prev = 0.05,
effects = "equal",
control = list(maxit = 1000)),
simplify = F))
View(results)
# Getting the estimated optimal coefficients from each list.
# Thereafter, use the checking function to determine the c-statistic
# and prevalence belonging to the estimated betas
results_check <- apply(sapply(results, '[[', 1), 2, checking, effects = "equal")
# What do the results look like?
apply(results_check, 1, summary)
# Taking the median of the coefficients,
# to reduce influence of potential outliers
par <- apply(sapply(results, '[[', 1), 1, median)
## Create validation dataset ##
n_val <- 100000 # setting n
X_val <- mvrnorm(n = n_val, mu = mu, Sigma = sigma) # create 10 predictor columns
dm_val <- cbind(1, X_val) # Putting the above in a data matrix, including intercept
# Same idea as the checking function, however, using the validation data:
checking_val <- function(par, effects = c("half strong", "equal")){
# Providing the beta0 and beta1-3 as specified in the par object.
if (effects == "half strong"){
# Half of the predictors is strong
dgm_par_val <- c(par[1], rep(par[2], round(n_pred/2)), rep((par[2]*3), round(n_pred/2)))
} else if (effects == "equal"){
# All predictors are equally strong
dgm_par_val <- c(par[1], rep(par[2], n_pred))
} else {
dgm_par_val <- c(par[1], rep(par[2], n_pred))
print("No effects supplied, so all are set to be equally strong")
}
p_val <- plogis(dm_val %*% dgm_par_val)
y_val <- rbinom(length(p_val),1,p_val)
# Obtain observed values
#obs_cstat <- c_stat2(preds = p_val, outcome = y_val) # obtain c-statistic based on p and y
obs_cstat <- fastAUC(p = p_val, y = y_val)
obs_prev <- mean(y_val) # THE OBSERVED PREVALENCE IS NOT A FUNCTION OF JUST THE INTERCEPT
c("cstat" = obs_cstat, "prev" = obs_prev)
}
## Use the checking function for the validation set:
checking_val(par = par, effects = "equal")
